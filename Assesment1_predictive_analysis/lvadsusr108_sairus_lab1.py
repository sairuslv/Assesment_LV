# -*- coding: utf-8 -*-
"""LVADSUSR108_Sairus_lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-pj9qcGyUT7ck6UO2hA4icTD1ao0jOHN
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import OneHotEncoder
from scipy import stats
import numpy as np

var_data=pd.read_csv('/content/expenses.csv')
var_data

var_data.describe()

var_data.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

# Identifying outliers using boxplots
fig, axs = plt.subplots(3, figsize=(10, 15))

sns.boxplot(var_data['age'], ax=axs[0]).set_title('Age Distribution')
sns.boxplot(var_data['bmi'], ax=axs[1]).set_title('BMI Distribution')
sns.boxplot(var_data['charges'], ax=axs[2]).set_title('Charges Distribution')

plt.tight_layout()
plt.show()





var_z_scores = stats.zscore(var_data['charges'])
var_abs_out = np.abs(var_z_scores)
var_filtered_entries = (var_abs_out < 3) # Filtering out outliers
var_data_cleaned = var_data[var_filtered_entries]
var_data_cleaned

#encoding
fUNC_encoder = OneHotEncoder(sparse=False, drop='first')
categorical_columns = ['sex', 'smoker', 'region']
var_encoded_data = fUNC_encoder.fit_transform(var_data_cleaned[categorical_columns])
encoded_df = pd.DataFrame(var_encoded_data, columns=fUNC_encoder.get_feature_names_out(categorical_columns))

#combing the data with above encoded one
var_data_final = var_data_cleaned.drop(categorical_columns, axis=1).reset_index(drop=True)
var_data_final = pd.concat([var_data_final, encoded_df], axis=1)
var_data_final

# Divide data into train and test
var_X = var_data_final.drop('charges', axis=1)
var_y = var_data_final['charges']
var_X_train, var_X_test, var_y_train, var_y_test = train_test_split(var_X, var_y, test_size=0.2, random_state=42)

var_X_train.shape

var_y_train.shape

model = LinearRegression()
model.fit(var_X_train, var_y_train)

# Model Evaluation
predictions = model.predict(var_X_test)
mse = mean_squared_error(var_y_test, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(var_y_test, predictions)

print(f'Mean Squared Error: {mse}')
print(f'Root Mean Squared Error: {rmse}')
print(f'R^2 Score: {r2}')

# Mean Squared Error (MSE) and Gradient Descent:
# MSE is also called cost function in regression problems.
# But GD is like an optimization algo to minimise the above MSE.
# It works like adjusting the parameters of the model.

# Learning Rate:
# It is a hyperparameter that controls weight wrt loss gradient.

# Derivatives and Partial Derivatives :
# The gradient descent algorithm uses partial derivatives of the MSE wrt each parameter
# to understand the direction in which each parameter should be adjusted to minimize the MSE.





















